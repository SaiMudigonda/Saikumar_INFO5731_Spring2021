{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiMudigonda/Saikumar_INFO5731_Spring2021/blob/main/In_class_exercise_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TahL04sVvR"
      },
      "source": [
        "# **The sixth in-class-exercise (20 points in total, 3/2/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyZITr8sjnh"
      },
      "source": [
        "## **1. Rule-based information extraction (10 points)**\n",
        "\n",
        "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "83fYWknzR8CK",
        "outputId": "6f357b35-96ae-4dd4-b118-95d5b118885e"
      },
      "source": [
        "from google.colab import files\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2db98759-8c37-420e-8f53-588488c1dcd9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2db98759-8c37-420e-8f53-588488c1dcd9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving TITLES.txt to TITLES.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "Q2qLGm0JhYn5",
        "outputId": "7a7b7030-7876-4698-d05f-8bae4724ab41"
      },
      "source": [
        "files = open(\"TITLES.txt\",'r',encoding = 'utf-8').read()\r\n",
        "files"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Energy and policy considerations for deep learning in NLP\\nJumpingÊNLPÊcurves: A review of natural language processing research\\nBRAT: a web-based tool forÊNLP-assisted text annotation\\nInstance weighting for domain adaptation inÊNLP\\nBERT rediscovers the classicalÊNLPÊpipeline\\nMultiword expressions: A pain in the neck forÊNLP\\nScatter search and localÊNLPÊsolvers: A multistart framework for global optimization\\nFLAIR: An easy-to-use framework for state-of-the-artÊNLP\\nAn LP/NLPÊbased branch and bound algorithm for convex MINLP optimization problems\\nÊPolo-like kinase 1 regulatesÊNlp, a centrosome protein involved in microtubule nucleation\\nVisualizing and understanding neural models inÊnlp\\nIntroducingÊNLP: Psychological skills for understanding and influencing people\\nLinguistically motivated large-scaleÊNLPÊwith C&C and Boxer\\nPolyglot: Distributed word representations for multilingualÊnlp\\nOptimum coordination of directional overcurrent relays using the hybrid GA-NLPÊapproach\\nParameter-efficient transfer learning forÊNLP\\nSemantically equivalent adversarial rules for debuggingÊnlpÊmodels\\nHow to train good word embeddings for biomedicalÊNLP\\nIntegratingÊNLPÊusing linked data\\nDeep learning for NLP (without magic)\\nGlobal optimum search for nonconvex NLP and MINLP problems\\nOvercoming barriers to NLP for clinical text: the role of shared tasks and the need for additional creative solutions\\nUniversal adversarial triggers for attacking and analyzing NLP\\nZemberek, an open source nlp framework for turkic languages\\nEraser: A benchmark to evaluate rationalized nlp models\\nAravec: A set of arabic word embedding models for use in arabic nlp\\nHow transferable are neural networks in nlp applications?\\nFreeLing 1.3: Syntactic and semantic services in an open-source NLP library\\nDiscovery of nitrate–CPK–NLP signalling in central nutrient–growth networks\\nNLP-Reduce: A naive but domainindependent natural language interface for querying ontologies\\nEudicot plant-specific sphingolipids determine host selectivity of microbial NLP cytolysins\\nBeyond accuracy: Behavioral testing of NLP models with CheckList\\nLanguage (technology) is power: A critical survey of\" bias\" in nlp\\nTokenization as the initial phase in NLP\\nITU Turkish NLP web service\\nAnalysis of the Wikipedia category graph for NLP applications\\nAlberto: Italian BERT language understanding model for NLP challenging tasks based on tweets\\nA broad-coverage collection of portable NLP components for building shareable analysis pipelines\\nTowards scalable and reliable capsule networks for challenging NLP applications\\nAn RLP23–SOBIR1–BAK1 complex mediates NLP-triggered immunity\\nThirty-five years of research on Neuro-Linguistic Programming. NLP research data base. State of the art or pseudoscientific decoration?\\nEvaluating nlp models via contrast sets\\nTectoMT: modular NLP framework\\nContextual lstm (clstm) models for large scale nlp tasks\\nGrammar sharing techniques for rule-based multilingual NLP systems\\nRetrieval-augmented generation for knowledge-intensive nlp tasks\\nTwitter as a lifeline: Human-annotated twitter corpora for NLP of crisis-related messages\\nThe esa nlp solver worhp\\nDo nlp models know numbers? probing numeracy in embeddings\\nAutomated risk identification using NLP in cloud based development environments\\nCombining click-stream data with NLP tools to better understand MOOC completion\\nAllennlp interpret: A framework for explaining predictions of nlp models\\nTowards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?\\nSemantic NLP-based information extraction from construction regulatory documents for automated compliance checking\\nNLP Techniques for Term Extraction and Ontology Population.\\nCoordinate regulation of the mother centriole component nlp by nek2 and plk1 protein kinases\\nThe NLP Toxin Family in Phytophthora sojae Includes Rapidly Evolving Groups That Lack Necrosis-Inducing Activity\\nDeep learning for Arabic NLP: A survey\\nA trainable summarizer with knowledge acquired from robust NLP techniques\\nAn introductory survey on attention mechanisms in NLP problems\\nInteracting TCP and NLP transcription factors control plant responses to nitrate availability\\nAn empirical investigation of statistical significance in nlp\\nA situated ontology for practical NLP\\nLarge‐scale DAE optimization using a simultaneous NLP formulation\\nDeep sentiment classification and topic discovery on novel coronavirus or covid-19 online discussions: Nlp using lstm recurrent neural network approach\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v28loZi9haKC",
        "outputId": "37ec5a70-982a-4148-cc68-a3ea6fa893cf"
      },
      "source": [
        "import re \r\n",
        "import string \r\n",
        "import nltk \r\n",
        "import spacy \r\n",
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import math \r\n",
        "from tqdm import tqdm \r\n",
        "\r\n",
        "from spacy.matcher import Matcher \r\n",
        "from spacy.tokens import Span \r\n",
        "from spacy import displacy \r\n",
        "\r\n",
        "pd.set_option('display.max_colwidth', 200)\r\n",
        "# load spaCy model\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "\r\n",
        "doc = nlp(files)\r\n",
        "\r\n",
        "for tok in doc: \r\n",
        "  print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)\r\n",
        "  \r\n",
        "pattern = [{'POS':'NOUN'}, \r\n",
        "           {'LOWER': 'clustering'}, \r\n",
        "           {'POS': 'PROPN'} \r\n",
        "           ]\r\n",
        "\r\n",
        "\r\n",
        "matcher = Matcher(nlp.vocab) \r\n",
        "matcher.add(\"matching_1\", None, pattern) \r\n",
        "matches = matcher(doc) \r\n",
        "span = doc[matches[100][1]:matches[100][2]]\r\n",
        "print(span.text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Energy --> nmod --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "policy --> conj --> NOUN\n",
            "considerations --> nsubj --> NOUN\n",
            "for --> prep --> ADP\n",
            "deep --> amod --> ADJ\n",
            "learning --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "NLP --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "JumpingÊNLPÊcurves --> ROOT --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "review --> nsubj --> NOUN\n",
            "of --> prep --> ADP\n",
            "natural --> amod --> ADJ\n",
            "language --> compound --> NOUN\n",
            "processing --> compound --> NOUN\n",
            "research --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "BRAT --> dobj --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "web --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "based --> amod --> VERB\n",
            "tool --> ROOT --> NOUN\n",
            "forÊNLP --> npadvmod --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "assisted --> amod --> VERB\n",
            "text --> compound --> NOUN\n",
            "annotation --> appos --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Instance --> compound --> NOUN\n",
            "weighting --> ROOT --> VERB\n",
            "for --> prep --> ADP\n",
            "domain --> compound --> NOUN\n",
            "adaptation --> pobj --> NOUN\n",
            "inÊNLP --> ROOT --> X\n",
            "\n",
            " -->  --> SPACE\n",
            "BERT --> nsubj --> PROPN\n",
            "rediscovers --> ROOT --> VERB\n",
            "the --> det --> DET\n",
            "classicalÊNLPÊpipeline --> nmod --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Multiword --> compound --> PROPN\n",
            "expressions --> dobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "pain --> ROOT --> NOUN\n",
            "in --> prep --> ADP\n",
            "the --> det --> DET\n",
            "neck --> pobj --> NOUN\n",
            "forÊNLP --> ROOT --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "Scatter --> compound --> PROPN\n",
            "search --> ROOT --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "localÊNLPÊsolvers --> conj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "multistart --> amod --> ADJ\n",
            "framework --> nsubj --> NOUN\n",
            "for --> prep --> ADP\n",
            "global --> amod --> ADJ\n",
            "optimization --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "FLAIR --> ROOT --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "An --> det --> DET\n",
            "easy --> amod --> ADJ\n",
            "- --> punct --> PUNCT\n",
            "to --> prep --> ADP\n",
            "- --> punct --> PUNCT\n",
            "use --> pobj --> NOUN\n",
            "framework --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "state --> nmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "of --> prep --> ADP\n",
            "- --> punct --> PUNCT\n",
            "the --> det --> DET\n",
            "- --> punct --> PUNCT\n",
            "artÊNLP --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "An --> det --> DET\n",
            "LP --> nmod --> PROPN\n",
            "/ --> punct --> SYM\n",
            "NLPÊbased --> amod --> VERB\n",
            "branch --> pobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "bound --> conj --> VERB\n",
            "algorithm --> dobj --> PROPN\n",
            "for --> prep --> ADP\n",
            "convex --> compound --> PROPN\n",
            "MINLP --> compound --> PROPN\n",
            "optimization --> compound --> NOUN\n",
            "problems --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "ÊPolo --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "like --> amod --> ADJ\n",
            "kinase --> nmod --> PROPN\n",
            "1 --> nummod --> NUM\n",
            "regulatesÊNlp --> ROOT --> NOUN\n",
            ", --> punct --> PUNCT\n",
            "a --> det --> DET\n",
            "centrosome --> compound --> NOUN\n",
            "protein --> appos --> NOUN\n",
            "involved --> acl --> VERB\n",
            "in --> prep --> ADP\n",
            "microtubule --> compound --> ADJ\n",
            "nucleation --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Visualizing --> ROOT --> VERB\n",
            "and --> cc --> CCONJ\n",
            "understanding --> conj --> VERB\n",
            "neural --> amod --> ADJ\n",
            "models --> dobj --> NOUN\n",
            "inÊnlp --> ROOT --> NUM\n",
            "\n",
            " -->  --> SPACE\n",
            "IntroducingÊNLP --> ROOT --> NUM\n",
            ": --> punct --> PUNCT\n",
            "Psychological --> amod --> ADJ\n",
            "skills --> intj --> NOUN\n",
            "for --> prep --> ADP\n",
            "understanding --> pobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "influencing --> conj --> VERB\n",
            "people --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Linguistically --> advmod --> ADV\n",
            "motivated --> ROOT --> VERB\n",
            "large --> amod --> ADJ\n",
            "- --> punct --> PUNCT\n",
            "scaleÊNLPÊwith --> nmod --> PROPN\n",
            "C&C --> dobj --> PROPN\n",
            "and --> cc --> CCONJ\n",
            "Boxer --> conj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "Polyglot --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "Distributed --> amod --> VERB\n",
            "word --> compound --> NOUN\n",
            "representations --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "multilingualÊnlp --> pobj --> NUM\n",
            "\n",
            " -->  --> SPACE\n",
            "Optimum --> amod --> ADJ\n",
            "coordination --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "directional --> amod --> ADJ\n",
            "overcurrent --> compound --> ADJ\n",
            "relays --> pobj --> NOUN\n",
            "using --> acl --> VERB\n",
            "the --> det --> DET\n",
            "hybrid --> amod --> ADJ\n",
            "GA --> compound --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "NLPÊapproach --> intj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Parameter --> npadvmod --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "efficient --> amod --> ADJ\n",
            "transfer --> compound --> NOUN\n",
            "learning --> dobj --> VERB\n",
            "forÊNLP --> ROOT --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "Semantically --> advmod --> ADV\n",
            "equivalent --> amod --> ADJ\n",
            "adversarial --> amod --> ADJ\n",
            "rules --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "debuggingÊnlpÊmodels --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "How --> advmod --> ADV\n",
            "to --> aux --> PART\n",
            "train --> ROOT --> VERB\n",
            "good --> amod --> ADJ\n",
            "word --> compound --> NOUN\n",
            "embeddings --> dobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "biomedicalÊNLP --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "IntegratingÊNLPÊusing --> nsubj --> VERB\n",
            "linked --> ROOT --> VERB\n",
            "data --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Deep --> amod --> ADJ\n",
            "learning --> ROOT --> VERB\n",
            "for --> prep --> ADP\n",
            "NLP --> pobj --> PROPN\n",
            "( --> punct --> PUNCT\n",
            "without --> prep --> ADP\n",
            "magic --> pobj --> NOUN\n",
            ") --> punct --> PUNCT\n",
            "\n",
            " -->  --> SPACE\n",
            "Global --> amod --> ADJ\n",
            "optimum --> amod --> ADJ\n",
            "search --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "nonconvex --> compound --> NOUN\n",
            "NLP --> pobj --> PROPN\n",
            "and --> cc --> CCONJ\n",
            "MINLP --> amod --> PROPN\n",
            "problems --> conj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Overcoming --> ROOT --> VERB\n",
            "barriers --> dobj --> NOUN\n",
            "to --> prep --> ADP\n",
            "NLP --> pobj --> PROPN\n",
            "for --> prep --> ADP\n",
            "clinical --> amod --> ADJ\n",
            "text --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "the --> det --> DET\n",
            "role --> dobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "shared --> amod --> VERB\n",
            "tasks --> pobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "the --> det --> DET\n",
            "need --> conj --> NOUN\n",
            "for --> prep --> ADP\n",
            "additional --> amod --> ADJ\n",
            "creative --> amod --> ADJ\n",
            "solutions --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Universal --> nmod --> PROPN\n",
            "adversarial --> amod --> ADJ\n",
            "triggers --> conj --> NOUN\n",
            "for --> prep --> ADP\n",
            "attacking --> pcomp --> VERB\n",
            "and --> cc --> CCONJ\n",
            "analyzing --> conj --> VERB\n",
            "NLP --> dobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "Zemberek --> conj --> PROPN\n",
            ", --> punct --> PUNCT\n",
            "an --> det --> DET\n",
            "open --> amod --> ADJ\n",
            "source --> appos --> NOUN\n",
            "nlp --> compound --> NOUN\n",
            "framework --> dobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "turkic --> compound --> ADJ\n",
            "languages --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Eraser --> ROOT --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "benchmark --> appos --> NOUN\n",
            "to --> aux --> PART\n",
            "evaluate --> relcl --> VERB\n",
            "rationalized --> amod --> VERB\n",
            "nlp --> compound --> NOUN\n",
            "models --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Aravec --> ROOT --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "set --> appos --> NOUN\n",
            "of --> prep --> ADP\n",
            "arabic --> amod --> ADJ\n",
            "word --> compound --> NOUN\n",
            "embedding --> compound --> NOUN\n",
            "models --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "use --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "arabic --> amod --> ADJ\n",
            "nlp --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "How --> advmod --> ADV\n",
            "transferable --> acomp --> ADJ\n",
            "are --> ROOT --> AUX\n",
            "neural --> amod --> ADJ\n",
            "networks --> nsubj --> NOUN\n",
            "in --> prep --> ADP\n",
            "nlp --> compound --> NOUN\n",
            "applications --> pobj --> NOUN\n",
            "? --> punct --> PUNCT\n",
            "\n",
            " -->  --> SPACE\n",
            "FreeLing --> ROOT --> PROPN\n",
            "1.3 --> nummod --> NUM\n",
            ": --> punct --> PUNCT\n",
            "Syntactic --> amod --> ADJ\n",
            "and --> cc --> CCONJ\n",
            "semantic --> conj --> ADJ\n",
            "services --> ROOT --> NOUN\n",
            "in --> prep --> ADP\n",
            "an --> det --> DET\n",
            "open --> amod --> ADJ\n",
            "- --> punct --> PUNCT\n",
            "source --> compound --> NOUN\n",
            "NLP --> compound --> PROPN\n",
            "library --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Discovery --> appos --> PROPN\n",
            "of --> prep --> ADP\n",
            "nitrate --> pobj --> NOUN\n",
            "– --> punct --> PUNCT\n",
            "CPK --> appos --> PROPN\n",
            "– --> punct --> PUNCT\n",
            "NLP --> ROOT --> PROPN\n",
            "signalling --> acl --> VERB\n",
            "in --> prep --> ADP\n",
            "central --> amod --> ADJ\n",
            "nutrient --> pobj --> NOUN\n",
            "– --> punct --> PUNCT\n",
            "growth --> compound --> NOUN\n",
            "networks --> ROOT --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "NLP --> compound --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "Reduce --> ROOT --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "naive --> amod --> ADJ\n",
            "but --> cc --> CCONJ\n",
            "domainindependent --> conj --> NOUN\n",
            "natural --> amod --> ADJ\n",
            "language --> compound --> NOUN\n",
            "interface --> appos --> NOUN\n",
            "for --> prep --> ADP\n",
            "querying --> amod --> VERB\n",
            "ontologies --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Eudicot --> compound --> PROPN\n",
            "plant --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "specific --> amod --> ADJ\n",
            "sphingolipids --> nsubj --> NOUN\n",
            "determine --> ROOT --> VERB\n",
            "host --> compound --> NOUN\n",
            "selectivity --> dobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "microbial --> compound --> ADJ\n",
            "NLP --> compound --> PROPN\n",
            "cytolysins --> pobj --> VERB\n",
            "\n",
            " -->  --> SPACE\n",
            "Beyond --> prep --> ADP\n",
            "accuracy --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "Behavioral --> amod --> ADJ\n",
            "testing --> nsubj --> NOUN\n",
            "of --> prep --> ADP\n",
            "NLP --> compound --> PROPN\n",
            "models --> pobj --> NOUN\n",
            "with --> prep --> ADP\n",
            "CheckList --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "Language --> conj --> PROPN\n",
            "( --> punct --> PUNCT\n",
            "technology --> appos --> NOUN\n",
            ") --> punct --> PUNCT\n",
            "is --> ROOT --> AUX\n",
            "power --> attr --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "critical --> amod --> ADJ\n",
            "survey --> attr --> NOUN\n",
            "of --> prep --> ADP\n",
            "\" --> punct --> PUNCT\n",
            "bias --> pobj --> NOUN\n",
            "\" --> punct --> PUNCT\n",
            "in --> prep --> ADP\n",
            "nlp --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "Tokenization --> ROOT --> PROPN\n",
            "as --> prep --> SCONJ\n",
            "the --> det --> DET\n",
            "initial --> amod --> ADJ\n",
            "phase --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "NLP --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "ITU --> compound --> PROPN\n",
            "Turkish --> amod --> ADJ\n",
            "NLP --> compound --> PROPN\n",
            "web --> compound --> NOUN\n",
            "service --> ROOT --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Analysis --> ROOT --> PROPN\n",
            "of --> prep --> ADP\n",
            "the --> det --> DET\n",
            "Wikipedia --> compound --> PROPN\n",
            "category --> compound --> NOUN\n",
            "graph --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "NLP --> compound --> PROPN\n",
            "applications --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Alberto --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "Italian --> amod --> ADJ\n",
            "BERT --> compound --> PROPN\n",
            "language --> compound --> NOUN\n",
            "understanding --> compound --> NOUN\n",
            "model --> appos --> NOUN\n",
            "for --> prep --> ADP\n",
            "NLP --> npadvmod --> PROPN\n",
            "challenging --> amod --> VERB\n",
            "tasks --> pobj --> NOUN\n",
            "based --> acl --> VERB\n",
            "on --> prep --> ADP\n",
            "tweets --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "A --> det --> DET\n",
            "broad --> amod --> ADJ\n",
            "- --> punct --> PUNCT\n",
            "coverage --> compound --> NOUN\n",
            "collection --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "portable --> amod --> ADJ\n",
            "NLP --> compound --> PROPN\n",
            "components --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "building --> pcomp --> VERB\n",
            "shareable --> amod --> ADJ\n",
            "analysis --> compound --> NOUN\n",
            "pipelines --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Towards --> prep --> ADV\n",
            "scalable --> amod --> ADJ\n",
            "and --> cc --> CCONJ\n",
            "reliable --> conj --> ADJ\n",
            "capsule --> conj --> NOUN\n",
            "networks --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "challenging --> pcomp --> VERB\n",
            "NLP --> compound --> PROPN\n",
            "applications --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "An --> det --> DET\n",
            "RLP23–SOBIR1–BAK1 --> nmod --> NOUN\n",
            "complex --> amod --> NOUN\n",
            "mediates --> ROOT --> VERB\n",
            "NLP --> npadvmod --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "triggered --> amod --> VERB\n",
            "immunity --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Thirty --> compound --> NUM\n",
            "- --> punct --> PUNCT\n",
            "five --> nummod --> NUM\n",
            "years --> appos --> NOUN\n",
            "of --> prep --> ADP\n",
            "research --> pobj --> NOUN\n",
            "on --> prep --> ADP\n",
            "Neuro --> compound --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "Linguistic --> compound --> PROPN\n",
            "Programming --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "NLP --> compound --> PROPN\n",
            "research --> compound --> NOUN\n",
            "data --> compound --> NOUN\n",
            "base --> ROOT --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "State --> nmod --> NOUN\n",
            "of --> prep --> ADP\n",
            "the --> det --> DET\n",
            "art --> pobj --> NOUN\n",
            "or --> cc --> CCONJ\n",
            "pseudoscientific --> conj --> ADJ\n",
            "decoration --> ROOT --> NOUN\n",
            "? --> punct --> PUNCT\n",
            "\n",
            " -->  --> SPACE\n",
            "Evaluating --> ROOT --> VERB\n",
            "nlp --> compound --> NOUN\n",
            "models --> dobj --> NOUN\n",
            "via --> prep --> ADP\n",
            "contrast --> compound --> NOUN\n",
            "sets --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "TectoMT --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "modular --> amod --> ADJ\n",
            "NLP --> compound --> PROPN\n",
            "framework --> ROOT --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Contextual --> amod --> PROPN\n",
            "lstm --> nmod --> NOUN\n",
            "( --> punct --> PUNCT\n",
            "clstm --> nmod --> NOUN\n",
            ") --> punct --> PUNCT\n",
            "models --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "large --> amod --> ADJ\n",
            "scale --> compound --> NOUN\n",
            "nlp --> compound --> NOUN\n",
            "tasks --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Grammar --> nsubj --> PROPN\n",
            "sharing --> ROOT --> VERB\n",
            "techniques --> dobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "rule --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "based --> amod --> VERB\n",
            "multilingual --> amod --> ADJ\n",
            "NLP --> compound --> PROPN\n",
            "systems --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Retrieval --> npadvmod --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "augmented --> amod --> VERB\n",
            "generation --> appos --> NOUN\n",
            "for --> prep --> ADP\n",
            "knowledge --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "intensive --> amod --> ADJ\n",
            "nlp --> amod --> NOUN\n",
            "tasks --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Twitter --> ROOT --> PROPN\n",
            "as --> prep --> SCONJ\n",
            "a --> det --> DET\n",
            "lifeline --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "Human --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "annotated --> amod --> VERB\n",
            "twitter --> compound --> NOUN\n",
            "corpora --> conj --> NOUN\n",
            "for --> prep --> ADP\n",
            "NLP --> pobj --> PROPN\n",
            "of --> prep --> ADP\n",
            "crisis --> npadvmod --> NOUN\n",
            "- --> punct --> PUNCT\n",
            "related --> amod --> VERB\n",
            "messages --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "The --> det --> DET\n",
            "esa --> nsubj --> PROPN\n",
            "nlp --> ROOT --> NOUN\n",
            "solver --> compound --> NOUN\n",
            "worhp --> dobj --> VERB\n",
            "\n",
            " -->  --> SPACE\n",
            "Do --> aux --> AUX\n",
            "nlp --> compound --> NOUN\n",
            "models --> nsubj --> NOUN\n",
            "know --> ROOT --> VERB\n",
            "numbers --> dobj --> NOUN\n",
            "? --> punct --> PUNCT\n",
            "probing --> ROOT --> VERB\n",
            "numeracy --> dobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "embeddings --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Automated --> amod --> VERB\n",
            "risk --> compound --> NOUN\n",
            "identification --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "NLP --> dobj --> PROPN\n",
            "in --> prep --> ADP\n",
            "cloud --> pobj --> NOUN\n",
            "based --> amod --> VERB\n",
            "development --> compound --> NOUN\n",
            "environments --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Combining --> ROOT --> VERB\n",
            "click --> compound --> ADJ\n",
            "- --> punct --> PUNCT\n",
            "stream --> compound --> NOUN\n",
            "data --> dobj --> NOUN\n",
            "with --> prep --> ADP\n",
            "NLP --> compound --> PROPN\n",
            "tools --> pobj --> NOUN\n",
            "to --> aux --> PART\n",
            "better --> advmod --> ADV\n",
            "understand --> advcl --> VERB\n",
            "MOOC --> compound --> PROPN\n",
            "completion --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Allennlp --> nsubj --> PROPN\n",
            "interpret --> ROOT --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "framework --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "explaining --> pcomp --> VERB\n",
            "predictions --> dobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "nlp --> compound --> NOUN\n",
            "models --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Towards --> prep --> ADP\n",
            "Faithfully --> advmod --> ADV\n",
            "Interpretable --> amod --> ADJ\n",
            "NLP --> compound --> PROPN\n",
            "Systems --> pobj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "How --> advmod --> ADV\n",
            "should --> aux --> VERB\n",
            "we --> nsubj --> PRON\n",
            "define --> acl --> VERB\n",
            "and --> cc --> CCONJ\n",
            "evaluate --> conj --> VERB\n",
            "faithfulness --> dobj --> NOUN\n",
            "? --> punct --> PUNCT\n",
            "\n",
            " -->  --> SPACE\n",
            "Semantic --> amod --> ADJ\n",
            "NLP --> npadvmod --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "based --> amod --> VERB\n",
            "information --> compound --> NOUN\n",
            "extraction --> ROOT --> NOUN\n",
            "from --> prep --> ADP\n",
            "construction --> nmod --> NOUN\n",
            "regulatory --> amod --> ADJ\n",
            "documents --> pobj --> NOUN\n",
            "for --> prep --> ADP\n",
            "automated --> amod --> VERB\n",
            "compliance --> compound --> NOUN\n",
            "checking --> pobj --> VERB\n",
            "\n",
            " -->  --> SPACE\n",
            "NLP --> compound --> PROPN\n",
            "Techniques --> ROOT --> PROPN\n",
            "for --> prep --> ADP\n",
            "Term --> compound --> PROPN\n",
            "Extraction --> nmod --> PROPN\n",
            "and --> cc --> CCONJ\n",
            "Ontology --> conj --> PROPN\n",
            "Population --> pobj --> PROPN\n",
            ". --> punct --> PUNCT\n",
            "\n",
            " -->  --> SPACE\n",
            "Coordinate --> amod --> VERB\n",
            "regulation --> nsubj --> NOUN\n",
            "of --> prep --> ADP\n",
            "the --> det --> DET\n",
            "mother --> pobj --> NOUN\n",
            "centriole --> compound --> PROPN\n",
            "component --> compound --> NOUN\n",
            "nlp --> ROOT --> PROPN\n",
            "by --> agent --> ADP\n",
            "nek2 --> pobj --> PROPN\n",
            "and --> cc --> CCONJ\n",
            "plk1 --> compound --> PROPN\n",
            "protein --> compound --> NOUN\n",
            "kinases --> conj --> VERB\n",
            "\n",
            " -->  --> SPACE\n",
            "The --> det --> DET\n",
            "NLP --> compound --> PROPN\n",
            "Toxin --> compound --> PROPN\n",
            "Family --> nsubj --> PROPN\n",
            "in --> prep --> ADP\n",
            "Phytophthora --> compound --> PROPN\n",
            "sojae --> pobj --> NOUN\n",
            "Includes --> ROOT --> VERB\n",
            "Rapidly --> advmod --> ADV\n",
            "Evolving --> ROOT --> PROPN\n",
            "Groups --> dobj --> PROPN\n",
            "That --> nsubj --> DET\n",
            "Lack --> relcl --> VERB\n",
            "Necrosis --> compound --> PROPN\n",
            "- --> punct --> PUNCT\n",
            "Inducing --> compound --> VERB\n",
            "Activity --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Deep --> amod --> ADJ\n",
            "learning --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "Arabic --> amod --> ADJ\n",
            "NLP --> pobj --> PROPN\n",
            ": --> punct --> PUNCT\n",
            "A --> det --> DET\n",
            "survey --> ROOT --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "A --> det --> DET\n",
            "trainable --> amod --> ADJ\n",
            "summarizer --> ROOT --> NOUN\n",
            "with --> prep --> ADP\n",
            "knowledge --> pobj --> NOUN\n",
            "acquired --> acl --> VERB\n",
            "from --> prep --> ADP\n",
            "robust --> amod --> ADJ\n",
            "NLP --> compound --> PROPN\n",
            "techniques --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "An --> det --> DET\n",
            "introductory --> amod --> ADJ\n",
            "survey --> ROOT --> NOUN\n",
            "on --> prep --> ADP\n",
            "attention --> compound --> NOUN\n",
            "mechanisms --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "NLP --> compound --> PROPN\n",
            "problems --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Interacting --> ROOT --> VERB\n",
            "TCP --> nmod --> PROPN\n",
            "and --> cc --> CCONJ\n",
            "NLP --> conj --> PROPN\n",
            "transcription --> compound --> NOUN\n",
            "factors --> nsubj --> NOUN\n",
            "control --> dobj --> VERB\n",
            "plant --> compound --> NOUN\n",
            "responses --> dobj --> NOUN\n",
            "to --> prep --> PART\n",
            "nitrate --> compound --> VERB\n",
            "availability --> pobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "An --> det --> DET\n",
            "empirical --> amod --> ADJ\n",
            "investigation --> ROOT --> NOUN\n",
            "of --> prep --> ADP\n",
            "statistical --> amod --> ADJ\n",
            "significance --> pobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "nlp --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "A --> det --> DET\n",
            "situated --> amod --> ADJ\n",
            "ontology --> ROOT --> NOUN\n",
            "for --> prep --> ADP\n",
            "practical --> amod --> ADJ\n",
            "NLP --> pobj --> PROPN\n",
            "\n",
            " -->  --> SPACE\n",
            "Large‐scale --> compound --> NOUN\n",
            "DAE --> compound --> PROPN\n",
            "optimization --> ROOT --> NOUN\n",
            "using --> acl --> VERB\n",
            "a --> det --> DET\n",
            "simultaneous --> amod --> ADJ\n",
            "NLP --> compound --> PROPN\n",
            "formulation --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n",
            "Deep --> amod --> ADJ\n",
            "sentiment --> compound --> NOUN\n",
            "classification --> dobj --> NOUN\n",
            "and --> cc --> CCONJ\n",
            "topic --> compound --> NOUN\n",
            "discovery --> conj --> NOUN\n",
            "on --> prep --> ADP\n",
            "novel --> compound --> ADJ\n",
            "coronavirus --> pobj --> PROPN\n",
            "or --> cc --> CCONJ\n",
            "covid-19 --> nmod --> PROPN\n",
            "online --> amod --> ADJ\n",
            "discussions --> conj --> NOUN\n",
            ": --> punct --> PUNCT\n",
            "Nlp --> ROOT --> PROPN\n",
            "using --> xcomp --> VERB\n",
            "lstm --> compound --> ADJ\n",
            "recurrent --> amod --> ADJ\n",
            "neural --> amod --> ADJ\n",
            "network --> compound --> NOUN\n",
            "approach --> dobj --> NOUN\n",
            "\n",
            " -->  --> SPACE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5445ea19eb23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matching_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq_7VGmrsum4"
      },
      "source": [
        "## **2. Domain-specific information extraction (10 points)**\n",
        "\n",
        "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
        "\n",
        "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
        "\n",
        "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
        "\n",
        "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
        "\n",
        "(4) companies, e.g., “Lexpredict LLC”\n",
        "\n",
        "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
        "\n",
        "(6) constraints, e.g., “no more than”\n",
        "\n",
        "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
        "\n",
        "(8) courts, e.g., “Supreme Court of New York”\n",
        "\n",
        "(9) CUSIP, e.g., “392690QT3”\n",
        "\n",
        "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
        "\n",
        "(11) definitions, e.g., “Term shall mean …”\n",
        "\n",
        "(12) distances, e.g., “fifteen miles”\n",
        "\n",
        "(13) durations, e.g., “ten years” or “thirty days”\n",
        "\n",
        "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
        "\n",
        "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
        "\n",
        "(16) percents and rates, e.g., “10%” or “50 bps”\n",
        "\n",
        "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
        "\n",
        "(18) ratios, e.g.,” 3:1” or “four to three”\n",
        "\n",
        "(19) regulations, e.g., “32 CFR 170”\n",
        "\n",
        "(20) trademarks, e.g., “MyApp (TM)”\n",
        "\n",
        "(21) URLs, e.g., “http://acme.com/”\n",
        "\n",
        "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
        "\n",
        "(23) persons, e.g., “John Doe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc7NtJrLx5tS",
        "outputId": "f497932a-5749-4b6a-f6fc-304505de6f88"
      },
      "source": [
        "# write your code here\r\n",
        "\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('all')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FadHxTvG-EP_",
        "outputId": "aa926fe7-d605-4626-b260-f2f905672cb1"
      },
      "source": [
        "from six.moves import urllib\r\n",
        "import pandas as pd\r\n",
        "!pip install lexnlp\r\n",
        "import lexnlp\r\n",
        "\r\n",
        "text = ''\r\n",
        "url = \"https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt\"\r\n",
        "txt_file = urllib.request.urlopen(url)\r\n",
        "\r\n",
        "for line in txt_file:\r\n",
        "  decoded_info = line.decode(\"utf-8\").replace('\\n', ' ')\r\n",
        "  if decoded_info: text = text + decoded_info"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lexnlp in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (3.5)\n",
            "Requirement already satisfied: us==2.0.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.0.2)\n",
            "Requirement already satisfied: dateparser==0.7.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.7.2)\n",
            "Requirement already satisfied: num2words==0.5.10 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.5.10)\n",
            "Requirement already satisfied: regex==2020.7.14 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2020.7.14)\n",
            "Requirement already satisfied: requests==2.24.0 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.24.0)\n",
            "Requirement already satisfied: numpy==1.19.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.19.1)\n",
            "Requirement already satisfied: reporters-db==2.0.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.5.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.5.1)\n",
            "Requirement already satisfied: datefinder-lexpredict==0.6.2.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.6.2.1)\n",
            "Requirement already satisfied: scikit-learn==0.23.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.23.1)\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (3.8.3)\n",
            "Requirement already satisfied: Unidecode==1.1.1 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (1.1.1)\n",
            "Requirement already satisfied: pycountry==20.7.3 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (20.7.3)\n",
            "Requirement already satisfied: joblib==0.14.0 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.14.0)\n",
            "Requirement already satisfied: pandas==0.24.2 in /usr/local/lib/python3.7/dist-packages (from lexnlp) (0.24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->lexnlp) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->lexnlp) (4.41.1)\n",
            "Requirement already satisfied: jellyfish==0.6.1 in /usr/local/lib/python3.7/dist-packages (from us==2.0.2->lexnlp) (0.6.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser==0.7.2->lexnlp) (2018.9)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser==0.7.2->lexnlp) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser==0.7.2->lexnlp) (2.8.1)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words==0.5.10->lexnlp) (0.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.24.0->lexnlp) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from reporters-db==2.0.3->lexnlp) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->lexnlp) (2.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->lexnlp) (4.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlNQUE7X-U2D",
        "outputId": "2c4bce69-7abf-4f10-b39d-8c06c1f48a66"
      },
      "source": [
        "#ACTS \r\n",
        "import lexnlp.extract.en.acts\r\n",
        "acts = lexnlp.extract.en.acts.get_act_list(text)\r\n",
        "if acts: \r\n",
        "  print(acts)\r\n",
        "else:\r\n",
        "  print('No Acts')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No Acts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFsJXjDM-XUf",
        "outputId": "86f86415-ca29-4d84-dd7e-cf125a68c15f"
      },
      "source": [
        "#AMOUNTS\r\n",
        "import lexnlp.extract.en.amounts\r\n",
        "print(list(lexnlp.extract.en.amounts.get_amounts(text)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Decimal('5.0'), Decimal('740.0'), Decimal('1843.0'), Decimal('2.0'), Decimal('1.0'), Decimal('4.0'), Decimal('2.0'), Decimal('1821.0'), Decimal('5.0'), Decimal('1.0'), Decimal('1840.0'), Decimal('3777.0'), Decimal('80.0'), Decimal('100.0'), Decimal('30.0'), Decimal('1839.0'), Decimal('741.0'), Decimal('22.0'), Decimal('1840.0'), Decimal('14000.0'), Decimal('120.0'), Decimal('1.0'), Decimal('1840.0'), Decimal('3.0'), Decimal('4.0'), Decimal('1.0'), Decimal('1.0'), Decimal('1840.0'), Decimal('2.0'), Decimal('1.0'), Decimal('361.0'), Decimal('1.0'), Decimal('307.0'), Decimal('6.0'), Decimal('604.0'), Decimal('1.0'), Decimal('2.0'), Decimal('418.0'), Decimal('422.0'), Decimal('7.0'), Decimal('34.0'), Decimal('41.0'), Decimal('167.0'), Decimal('742.0'), Decimal('3.0'), Decimal('112.0'), Decimal('207.0'), Decimal('3.0'), Decimal('338.0'), Decimal('424.0'), Decimal('5.0'), Decimal('26.0'), Decimal('13.0'), Decimal('235.0'), Decimal('8.0'), Decimal('693.0'), Decimal('4.0'), Decimal('1821.0'), Decimal('167.0'), Decimal('2.0'), Decimal('2.0'), Decimal('216.0'), Decimal('3.0'), Decimal('66.0'), Decimal('4.0'), Decimal('130.0'), Decimal('29.0'), Decimal('2.0'), Decimal('241.0'), Decimal('2.0'), Decimal('332.0'), Decimal('2.0'), Decimal('422.0'), Decimal('9.0'), Decimal('112.0'), Decimal('743.0'), Decimal('9.0'), Decimal('39.0'), Decimal('14000.0'), Decimal('1840.0'), Decimal('744.0'), Decimal('5.0'), Decimal('182.0'), Decimal('3.0'), Decimal('368.0'), Decimal('1.0'), Decimal('397.0'), Decimal('6.0'), Decimal('604.0'), Decimal('1.0'), Decimal('1821.0'), Decimal('167.0'), Decimal('745.0'), Decimal('4.0'), Decimal('746.0'), Decimal('4.0'), Decimal('210.0'), Decimal('46.0'), Decimal('747.0'), Decimal('5.0'), Decimal('5.0'), Decimal('740.0'), Decimal('1843.0'), Decimal('284.0'), Decimal('2019.0'), Decimal('9.0'), Decimal('1.0'), Decimal('55.0'), Decimal('266.0'), Decimal('271.0'), Decimal('1876.0'), Decimal('2.0'), Decimal('47.0'), Decimal('362.0'), Decimal('376.0'), Decimal('1872.0'), Decimal('3.0'), Decimal('45.0'), Decimal('329.0'), Decimal('334.0'), Decimal('1871.0'), Decimal('4.0'), Decimal('31.0'), Decimal('526.0'), Decimal('527.0'), Decimal('1858.0'), Decimal('5.0'), Decimal('21.0'), Decimal('333.0'), Decimal('335.0'), Decimal('1852.0'), Decimal('6.0'), Decimal('8.0'), Decimal('145.0'), Decimal('147.0'), Decimal('1857.0'), Decimal('7.0'), Decimal('65.0'), Decimal('256.0'), Decimal('258.0'), Decimal('3.0'), Decimal('1880.0'), Decimal('8.0'), Decimal('4.0'), Decimal('913.0'), Decimal('914.0'), Decimal('1887.0'), Decimal('9.0'), Decimal('103.0'), Decimal('464.0'), Decimal('1936.0'), Decimal('3.0'), Decimal('1.0'), Decimal('9.0'), Decimal('39.0'), Decimal('1828.0'), Decimal('2.0'), Decimal('2.0'), Decimal('5.0'), Decimal('182.0'), Decimal('1837.0'), Decimal('2.0'), Decimal('3.0'), Decimal('9.0'), Decimal('108.0'), Decimal('1812.0'), Decimal('6.0'), Decimal('1.0'), Decimal('2.0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w_s-fbx-gCk",
        "outputId": "b5867d92-0c1e-4ac1-b07c-6d8901ba22ee"
      },
      "source": [
        "#CITATIONS\r\n",
        "import lexnlp.extract.en.citations\r\n",
        "print(list(lexnlp.extract.en.citations.get_citations(text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(5, 'Ala.', 'Alabama Reports', 740, None, None, None), (5, 'Ala.', 'Alabama Reports', 740, '1843', None, None), (55, 'Ala.', 'Alabama Reports', 266, '271', None, None), (47, 'Ala.', 'Alabama Reports', 362, '376', None, None), (45, 'Ala.', 'Alabama Reports', 329, '334', None, None), (31, 'Ala.', 'Alabama Reports', 526, '527', None, None), (21, 'Ala.', 'Alabama Reports', 333, '335', None, None), (8, 'Cal.', 'California Reports', 145, '147', None, None), (65, 'Ala.', 'Alabama Reports', 256, '258', None, None), (4, 'S.W.', 'South Western Reporter', 913, '914', None, None), (103, 'A.L.R.', 'American Law Reports', 464, None, None, None), (9, 'Cow.', \"Cowen's Reports\", 39, None, None, None), (5, 'Port.', 'Alabama Reports, Porter', 182, None, None, None), (9, 'Johns.', \"Johnson's Reports\", 108, None, None, None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkaRgsMP-lT6",
        "outputId": "3955b43f-f3bc-4127-b8f3-c9b00094658e"
      },
      "source": [
        "#COMPANIES\r\n",
        "import lexnlp.extract.en.entities.nltk_re\r\n",
        "print(list(lexnlp.extract.en.entities.nltk_re.get_companies(text)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Lehman, Durr Co, (18055, 18073)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdaZ1Ua8-p0c",
        "outputId": "bd0d6023-ece1-4e1b-a0d8-9aa15885076e"
      },
      "source": [
        "#CONDITIONS\r\n",
        "import lexnlp.extract.en.conditions\r\n",
        "print(list(lexnlp.extract.en.conditions.get_conditions(text)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('until', '4 Cases that cite this headnote\\r \\r [2]\\r Creditors’ Remedies\\r Lien and Priority\\r Under St.1821, prohibiting a levy on a crop', ''), ('until', 'on a growing crop, nor does such lien attach', ''), ('if', 'It was proved by the claimants, by the production of a written contract, that Harrison, on the twenty-second of May, 1840, in consideration that the claimants were involved, as indorsers for Burton & Harrison of Sumter county, and were then exposed to an execution, amounting to upwards of fourteen thousand dollars, bargained and sold to the claimants all his growing crop of cotton &c., consisting of one hundred and twenty acres, &c. Allen Harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers;', ''), ('when', 'The claimants came from Tennessee, (where they resided) about the first of September, 1840, bringing with them three or four white laborers, and took possession of the crop and slaves, and with the latter, and white laborers, gathered the cotton, prepared it for market, and', ''), ('if', 'The court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that Harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but', ''), ('when', 'it was not, and the lien of the fieri facias would have attached upon it,', ''), ('if', 'gathered, yet', ''), ('not subject to', 'the claimants obtained possession on the first of September, and controlled the gathering of the crop, then no lien attached, and it was', ''), ('until', 'Rep, 693;] and', ''), ('until', '167,] which declares it to be lawful to levy an execution on a planted crop,', ''), ('if', 'It is admitted that the contract between the defendant in execution, and the claimants, was in good faith,', ''), ('when', 'The defendant in execution might at any time have divested the interest which the contract vested in the claimants, by discharging their liability as his indorsers, or a judgment creditor might have satisfied the lien, and', ''), ('unless', 'We will then consider the writing under which the claimants assert a right, as a mortgage with a power to take possession any time during the year,', ''), ('if', 'Conceding the truth of the facts stated in the bill of exceptions, and we think it will not follow, that the possession of the claimants is a nullity, and that the case must be considered as', ''), ('if', 'The contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity, and', ''), ('if', 'they took possession of it in the absence of the grantor, (though without his consent,)', ''), ('if', 'he subsequently acquiesced in it, the inference would be,', ''), ('subject to', 'Mr. Dane, in remarking upon this point, says, “The American editor of Bacon’s Abridgment, says, ‘Wheat growing in the ground is a chattel, and', ''), ('until', 'The first section of the act of 1821, “To prevent sheriffs and other officers from levying executions in certain cases, enacts, that “It shall not be lawful for any sheriff or other officer, to levy a writ of fieri facias or other execution on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('until', 'Now here is an express inhibition to levy an execution on a crop while it remains on, or in the ground, and', ''), ('until', 'If so, the act cited, will only have the effect of keeping the right to levy it in abeyance', ''), ('if', 'The lien and the right to levy are intimately connected, and', ''), ('until', 'That it was competent for the legislature to have made it unlawful to levy an execution on particular property,', ''), ('until', 'If the object was merely to suspend the sale,', ''), ('as soon as', 'The idea that the lien attached upon the planted crop', ''), ('until', 'the execution was delivered to the sheriff, though the right to levy it was postponed', ''), ('if', 'They do not refer to the lien,', ''), ('until', 'they did they would postpone it', ''), ('until', 'the crop was gathered; but it is the levy they relate to and postpone', ''), ('until', '**4 The right to levy an execution on a planted crop, then, being expressly taken away by the statute, the lien which is connected with and consequent upon that right, never attaches', ''), ('if', 'The circuit judge may have mistaken the law in supposing that the contract was a sale, but', ''), ('when', 'There is no assumption of any material fact in the charge; but the possession of the claimant, the time', ''), ('if', 'acquired, the gathering of the crop, &c., are all referred to the determination of the jury; who are instructed,', ''), ('until', '**4 The statute which presents the question before the court is, that “it shall not be lawful for any sheriff or other officer to levy a writ of fieei facias or other execution, on the planted crop of a debtor, or person against whom an execution may issue,', ''), ('subject to', 'The policy of the State, as indicated by these statutes, is undeniably that all the property of a debtor, real and personal, to which he has a legal title, shall be', ''), ('until', 'The mischief which the statute designed to remedy was, the sacrifice which would be necessarily made by the sale of an immature crop: the statute enables the debtor to retain it', ''), ('if', '**5', ''), ('until', 'The sheriff is forbidden to levy on a “planted crop”', ''), ('if', 'Now,', ''), ('until', 'This, I feel a thorough conviction, was not the intention of the legislature; but that it was to secure him from loss, by prohibiting a levy and sale of the crop,', ''), ('when', 'it was gathered,', ''), ('subject to', 'Growing crops as', ''), ('subject to', '464\\r Generally, at common law, growing crops raised by annual planting, while still attached to the soil, are regarded as personal chattels,', ''), ('where', 'And', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6J04jun-uTY",
        "outputId": "411a3b5c-5ad0-4275-edfb-a49ad1bc2ed3"
      },
      "source": [
        "#CONSTRAINTS\r\n",
        "import lexnlp.extract.en.constraints\r\n",
        "print(list(lexnlp.extract.en.constraints.get_constraints(text)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('after', 'on a growing crop, nor does such lien attach until', ''), ('after', '', ' and that alias and pluries fieri facias’, issued regularly up to the time levy was made; that the cotton levied on was growed on the plantation of harrison, and cultivated by the hands in his service.'), ('first of', 'the claimants came from tennessee, (where they resided) about the', ''), ('first of', 'the court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but if it was not, and the lien of the fieri facias would have attached upon it, when gathered, yet if the claimants obtained possession on the', ''), ('after', 'it merely inhibits the levy, but the lien attaches, and a levy and sale may be made', ''), ('more than', 'taking this to be clear *744 law, and it will be seen, that the defendant in execution at the time of the levy had nothing', ''), ('before', 'it has been frequently mooted whether, at common law, corn, &c.,', ''), ('before', '**4 the statute which presents the question', ''), ('after', 'now, if the view taken by the majority of the court, is correct, the right secured to the plaintiff in execution, of levying on the crop', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', ''), ('before', 'tried', '')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i23ukfyD-zPc",
        "outputId": "669a1d55-0b43-4e24-8574-3fbd63650b02"
      },
      "source": [
        "import lexnlp.extract.en.copyright\r\n",
        "print(list(lexnlp.extract.en.copyright.get_copyright(text)))#COPYRIGHTS"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('©', '2019', 'Thomson Reuters. No')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_14yl8b_D16",
        "outputId": "17f43c22-d535-4e79-be80-58b317c88d80"
      },
      "source": [
        "import lexnlp.extract.en.cusip\r\n",
        "cusip = list(lexnlp.extract.en.cusip.get_cusip(text))\r\n",
        "if cusip:\r\n",
        "  print(cusip)\r\n",
        "else:\r\n",
        "  print('No Cusip')#CUSIP"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No Cusip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP35HC_p_KFR",
        "outputId": "7f5d7c42-1cd5-4db8-f386-976b767982b9"
      },
      "source": [
        "#DATES\r\n",
        "import lexnlp.extract.en.dates\r\n",
        "print(list(lexnlp.extract.en.dates.get_dates(text)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[datetime.date(2021, 6, 1), datetime.date(1840, 11, 1), datetime.date(1839, 10, 1), datetime.date(1840, 9, 1), datetime.date(1840, 5, 1), datetime.date(1840, 5, 1), datetime.date(2021, 12, 1), datetime.date(2021, 12, 1), datetime.date(2021, 1, 1), datetime.date(2021, 1, 1), datetime.date(2021, 1, 1), datetime.date(2021, 3, 21), datetime.date(2021, 6, 1), datetime.date(2021, 7, 1), datetime.date(2021, 11, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaKEOBbs_N_6",
        "outputId": "c04a54fe-dd98-4ffc-e9c2-a8c7de5650bc"
      },
      "source": [
        "import lexnlp.extract.en.definitions\r\n",
        "definitions = list(lexnlp.extract.en.definitions.get_definitions(text))\r\n",
        "if definitions:\r\n",
        "  print(definitions)\r\n",
        "else:\r\n",
        "  print('No definitions')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No definitions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcefX0YC_SG2",
        "outputId": "29049d35-e0f3-4e2c-da01-da384ead2b4b"
      },
      "source": [
        "#DISTANCES\r\n",
        "import lexnlp.extract.en.distances\r\n",
        "distances = list(lexnlp.extract.en.distances.get_distances(text))\r\n",
        "if distances:\r\n",
        "  print(distances)\r\n",
        "else:\r\n",
        "  print('No distances')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No distances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjEoSprm_WyN",
        "outputId": "de654d45-8707-437e-90d4-bd3ea7b3c0ba"
      },
      "source": [
        "import lexnlp.extract.en.durations\r\n",
        "print(list(lexnlp.extract.en.durations.get_durations(text)))#DURATIONS\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('second', Decimal('20.0'), Decimal('0.0002')), ('year', Decimal('6.0'), Decimal('2190.0'))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN8Ix1AI_0Ke"
      },
      "source": [
        "import lexnlp.extract.en.geoentities\r\n",
        "geo_df = pd.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-lexnlp/master/test_data/lexnlp/extract/en/tests/test_geoentities/geoentities.csv\")\r\n",
        "geo_config_data = []\r\n",
        "for _, row in geo_df.iterrows():\r\n",
        "  c = lexnlp.extract.en.dict_entities.entity_config(row[\"entity_id\"], row[\"name\"],row[\"category\"])\r\n",
        "  geo_config_data.append(c)\r\n",
        "for entity, alias in lexnlp.extract.en.geoentities.get_geoentities(text, geo_config_data):\r\n",
        "  print(\"entity=\", entity)\r\n",
        "  print(\"alias=\", alias)#GEOGRAPHIC LOCATIONS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWcKPtzM_4_p",
        "outputId": "c7a6b424-1242-4f08-8c67-b6954babc2a9"
      },
      "source": [
        "#MONEY\r\n",
        "import lexnlp.extract.en.money\r\n",
        "print(list(lexnlp.extract.en.money.get_money(text)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(Decimal('100.0'), 'USD'), (Decimal('14000.0'), 'USD'), (Decimal('14000.0'), 'USD')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLVv-mcyAPpl",
        "outputId": "7cdfffe4-64d1-4ba7-a1f6-925ce69836f8"
      },
      "source": [
        "import lexnlp.extract.en.percents\r\n",
        "percents = list(lexnlp.extract.en.percents.get_percents(text))\r\n",
        "if percents:\r\n",
        "  print(percents)\r\n",
        "else:\r\n",
        "  print('No percents') #PERCENTS\r\n",
        "\r\n",
        "import lexnlp.extract.en.pii\r\n",
        "pii = list(lexnlp.extract.en.pii.get_pii(text))\r\n",
        "if pii:\r\n",
        "  print(pii)\r\n",
        "else:\r\n",
        "  print('No pii')#PI\r\n",
        "\r\n",
        "import lexnlp.extract.en.ratios\r\n",
        "ratios = list(lexnlp.extract.en.ratios.get_ratios(text))\r\n",
        "if ratios:\r\n",
        "  print(ratios)\r\n",
        "else:\r\n",
        "  print('No ratios')#RATIOS\r\n",
        "\r\n",
        "import lexnlp.extract.en.regulations\r\n",
        "regulations = list(lexnlp.extract.en.regulations.get_regulations(text))\r\n",
        "if regulations:\r\n",
        "  print(regulations)\r\n",
        "else:\r\n",
        "  print('No Regulations')#REGULATIONS\r\n",
        "\r\n",
        "\r\n",
        "import lexnlp.extract.en.trademarks\r\n",
        "trademarks = list(lexnlp.extract.en.trademarks.get_trademarks(text))\r\n",
        "if trademarks:\r\n",
        "  print(trademarks)\r\n",
        "else:\r\n",
        "  print('No trademarks')#TARDEMARKS\r\n",
        "\r\n",
        "import lexnlp.extract.en.urls\r\n",
        "urls = list(lexnlp.extract.en.urls.get_urls(text))\r\n",
        "if urls:\r\n",
        "  print(urls)\r\n",
        "else:\r\n",
        "  print('No urls')#URLS\r\n",
        "\r\n",
        "!pip install pyap\r\n",
        "import pyap\r\n",
        "addresses = pyap.parse(text, country='US')\r\n",
        "for address in addresses:\r\n",
        "  print(address)\r\n",
        "  print(address.as_dict())#ADDRESSES\r\n",
        "\r\n",
        "import spacy\r\n",
        "spacy_nlp  = spacy.load('en_core_web_sm')\r\n",
        "doc = spacy_nlp(text.strip())\r\n",
        "person = set()\r\n",
        "for i in doc.ents:\r\n",
        "  entry = str(i.lemma_).lower()\r\n",
        "  text_sample = text.replace(str(i).lower(), \"\")\r\n",
        "  if i.label_ == \"PERSON\":\r\n",
        "    person.add(entry.title())\r\n",
        "person#PERSON"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No percents\n",
            "No pii\n",
            "No ratios\n",
            "No Regulations\n",
            "No trademarks\n",
            "No urls\n",
            "Requirement already satisfied: pyap in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "5 Ala. 740 Supreme Court of Alabama. ADAMS\n",
            "{'full_address': '5 Ala. 740 Supreme Court of Alabama. ADAMS', 'full_street': '5 Ala. 740 Supreme Court', 'street_number': '5', 'street_name': 'Ala. 740 Supreme', 'street_type': 'Court', 'route_id': None, 'post_direction': None, 'floor': None, 'building_id': None, 'occupancy': None, 'city': 'of Alabama. ADA', 'region1': 'MS', 'postal_code': None, 'country_id': 'US', 'match_start': 1, 'match_end': 44}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A. Quit - Claim',\n",
              " 'Allen Harrison',\n",
              " 'Austin',\n",
              " 'Austin V. Sawyer',\n",
              " 'Bibb',\n",
              " 'Booker',\n",
              " 'C. J.',\n",
              " 'Case',\n",
              " 'Cohen',\n",
              " 'Dane',\n",
              " 'Dewey',\n",
              " 'Doughty',\n",
              " 'Edwards',\n",
              " 'Elliott V. Mayfield',\n",
              " 'Evans',\n",
              " 'Hon',\n",
              " 'Hurtell',\n",
              " 'J.',\n",
              " 'Jacob S. Cohen',\n",
              " 'Janney',\n",
              " 'John D. Cunningham',\n",
              " 'Jones',\n",
              " 'Jun Term',\n",
              " 'L. Whitlock',\n",
              " 'Lamar',\n",
              " 'Lampley',\n",
              " 'Lien',\n",
              " 'Lien Attach',\n",
              " 'M. J. Saffold',\n",
              " 'Mansony',\n",
              " 'Marshall',\n",
              " 'Mayfield',\n",
              " 'Mckenzie',\n",
              " 'Perkins',\n",
              " 'Poole',\n",
              " 'R. H. Smith',\n",
              " 'Sawyer',\n",
              " 'St.1821',\n",
              " 'Stewart',\n",
              " 'Stewart V. Doughty',\n",
              " 'W.',\n",
              " 'W. G. Jones',\n",
              " 'Whipple V. Foot',\n",
              " 'Wood'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}